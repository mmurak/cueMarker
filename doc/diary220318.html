<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ja" lang="ja">
  <head>
    <title>Diary 220318</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" >
    <link rel="shortcut icon" href="../kiwi-bird.png">
  </head>
  <body">

<h2>はじめに</h2>

<p>　パト先生（@patosensei105）の、「<span style="color: green;">リスニングをしながら聞き取れない箇所が出てきたらボタンを押し続けることで後からスクリプトのその箇所に下線なり何らかの印がついているリスニング教材アプリ</span>」というツイートを目にし、興味深いお題だと感じたので、色々と可能性を模索しているところ（技術と妥協がせめぎ合いそうな素晴らしいネタ振り、ありがとうございます＞パト先生）。</p>

<h2>最初に考えたこと</h2>

<p>　このお題の難しいところは2種類のタイミング情報を管理し、そのギャップを（完全にとは行かないまでも）埋める必要があるというところにあると思った。その2種類のタイミング情報とは、</p>

<ol>
<li>音声再生時にスクリプトのどの部分を再生しているのかというタイミング情報</li>
<li>ユーザーが音声を聞き、「聞き取れない！」と判断し、「ボタン押さなあかん〜！」と考えてから実際にボタンを押すまでのタイミングのずれ情報</li>
</ol>


<h4>(1)のタイミングについて。</h4>

<p>　当たり前の話だけど、文を構成する各単語／音節は常に一定スピードで流れているわけではなく、10語からなる1文を10秒で発話するからといって、各単語すべてが1秒で発音されているわけではない。
このため、何らかの対応を取る必要がある。</p>

<p>　ここは自動化ができそうにも思えたが、当面は目的にかなうレベルのものにはならないだろうという結論に達した。その理由は以下の通り。</p>

<p id="l01">　AIを活用した自動化のソリューションというのは、<a href="#l01" title="実際のところ、例えばotter.aiはスピーチtoテキスト変換だけじゃなく、各単語の発話開始／終了タイミングまで押さえてる、、、けどライセンス上、リバースエンジニアリングが禁止されているためそれを流用するというズルもできない。">ネタとしては興味深い</a>ものの、単語や音節レベルのタイミングを取得したとしても、細かすぎて、(2)のおおらかなタイミングと比べるとギャップが大きすぎるはず、、、おそらくはイントネーションを構成するフレーズくらいの粒度でまとめる必要があるんじゃないだろうか（もっとも単語のタイミングが分かれば、フレーズのタイミングも分かるんだが、そうなると単なるスピーチtoテキストじゃなく、「意味の理解」という厄介な分野に入り込んでしまい、現時点でのAIにとっては荷が重すぎる話になる）。</p>

<p>　ということで、ここは人力で解決することにし、如何に簡単にタイミングを指定できるのかという方向で進めてみることにした。</p>

<h4>(1).の結論</h4>

<p>　とりあえず可能性を探るためのアプリという前提で、音声とスクリプト（テキストファイル）を読み込み、テキスト上にキューマーク（時間という属性を有している▲マーク）を打ち込むものを作ってみた。これによりスクリプトの要所要所にタイミング情報を埋め込み、スクリプトを複数のセグメントに分割できるようにする。</p>

<div style="text-align: center"><img src="../img/cueMarker01.png" width="600"><br/>↑これがcueMarker。使い方は<a href="./cueMarkerDoc.html">こちら</a>。</div>


<h4>(2). のタイミングについて</h4>

<p>　cueMarkerによってスクリプトを複数のセグメントに分割し、各セグメントの開始時間を特定できるようになったため、ユーザーが「聞き取れない！」ボタンを押したタイミングで再生されているセグメントを認識できるようにはなる。ただ、いくら正確なタイミングを都合のよい単位で作り出せても、ユーザーの反応速度は制御できないため、何らかの妥協が必要になる。</p>

<p>　そこで「細けぇこたぁいいんだよっ！」っと目をつぶって、まずユーザーがボタンを押した際に再生されているセグメントを赤字で表示するようにしようと決めた。ただ、ユーザーのクリック操作が遅れ、再生が次のセグメントに移っていることは普通にありそうなので、1つ前のセグメントにもちょっと薄めの色を着色するようにした。</p>

<div style="text-align: center">これらを考慮して作成したのがユースケース弐号機↓<br/>使用感のお試しは<a href="../usecase02.html">こちら</a>。<br/>そして、cueMarkerのデータを使用するための手順は<a href="./usecase02Doc.html">こちら</a>。<br/><img src="../img/usecase002Sample.png" width="600"></div>

<p>　使い方は、「設問」の右にあるドロップダウンボックスから問題（ファイル）を選択し、「再生」を押し、聞き取れない部分が出てきたら「分からんからマーク！」ボタンを押すだけ。再生速度も0.25〜2倍速まで変更できるようにしてみた。</p>

<p>　さらに「分からんからマーク！」ボタンを押した際の動作を視認できるよう、再生前（&amp;途中）でもスクリプトの表示／非表示を切り替えられる「Show/Hide」ボタンを追加し、スクリプトの文字部分をクリックすると、該当部分の音声が再生されるようにしてみた。</p>

<h4>データ作成に関する考察</h4>

<p>　セグメントの間隔をどれくらいの長さにするのが妥当なのかは、音声＋スクリプトのセットを色々と区切ってみて試行錯誤する必要がありそう。</p>

<p>　なお例文はまず、ユーザーがリスニングで引っかかりそうな部分（？）を絞り込める程度の長さに分けようと、複数の単語をまとめてみた。例文は、枕草子のオーパーツ、いや「かき氷」が登場する「あてなるもの」の段（「The Pillow Book of Sei Shonagon」Ivan Morrisより）。</p>

<div style="text-align: center"><img src="../img/ElegantThings.png" width="600"></div>

<p>　次の案として、複数の単語のまとまりを基準にしてキューを置いていくのではなく、タイミングキュー自体を一定間隔（1秒）で配置してみるというデータも作ってみた。この場合、単語の途中等にキューが入る場合もあるため、単語がセグメント境界をまたがっている場合（つまり単語の途中にキューマークが入っている場合）、その単語も当該セグメントのものと考え、着色するようにしている（ただ、スクリプトをクリックしても音声はセグメントのとおりに再生されるのがちょっとタコ）。例文は同じく枕草子から、清少納言のお気に入りとも考えられる源成信公の耳の良さを語る「成信の中将こそ」の段（「The Pillow Book of Sei Shonagon」Ivan Morrisより）。</p>

<div style="text-align: center"><img src="../img/CaptainNarinobu.png" width="600"><br/>※ キューの位置は割といい加減に投入してるので、「t▲here」等、音節境界にもなってません。（^^;）</div>

<p>〜とりあえず今はこんな感じ〜</p>

</body>
</html>
